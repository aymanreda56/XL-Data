{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustained-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.ml\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "\n",
    "import sys; sys.path.append(\"../\")\n",
    "from DataPreprocessing.DataPreprocessing import read_data,encode_categ_features,remove_useless_col\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d96bafd",
   "metadata": {},
   "source": [
    "# Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continent-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local')\n",
    "spark = SparkSession.builder.appName(\"InstallationsPrediction\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "638ae28a",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handled-basketball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting outliers...\n",
      "Number of outliers in Rating: 0 (0.00%)\n",
      "Number of outliers in Rating Count: 388994 (16.82%)\n",
      "Number of outliers in Minimum Installs: 260554 (11.27%)\n",
      "Number of outliers in Maximum Installs: 407678 (17.63%)\n",
      "Number of outliers in Price: 44943 (1.94%)\n",
      "Number of rows before removing outliers: 2312944\n",
      "Number of rows after removing those having more than 1 outlier in its columns: 1961130\n",
      "Removing useless columns...\n",
      "Handling missing values...\n",
      "Total Number of rows : 1961130\n",
      "Number of rows after dropping nulls: 1954764\n",
      "Total Number of rows : 1954764\n",
      "Total Number of rows : 1954764\n",
      "Converting size to bytes...\n",
      "Converted all sizes to Bytes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('App Name', 'string'),\n",
       " ('App Id', 'string'),\n",
       " ('Category', 'string'),\n",
       " ('Rating', 'float'),\n",
       " ('Rating Count', 'int'),\n",
       " ('Installs', 'string'),\n",
       " ('Minimum Installs', 'int'),\n",
       " ('Maximum Installs', 'int'),\n",
       " ('Free', 'string'),\n",
       " ('Price', 'float'),\n",
       " ('Size', 'string'),\n",
       " ('Minimum Android', 'string'),\n",
       " ('Developer Id', 'string'),\n",
       " ('Developer Email', 'string'),\n",
       " ('Released', 'string'),\n",
       " ('Last Updated', 'string'),\n",
       " ('Content Rating', 'string'),\n",
       " ('Ad Supported', 'string'),\n",
       " ('In App Purchases', 'string'),\n",
       " ('Editors Choice', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= read_data(spark, features='all')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c276bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Category', 'string'),\n",
       " ('Maximum Installs', 'int'),\n",
       " ('Free', 'string'),\n",
       " ('Price', 'float'),\n",
       " ('Size', 'string'),\n",
       " ('Minimum Android', 'string'),\n",
       " ('Developer Id', 'string'),\n",
       " ('Developer Email', 'string'),\n",
       " ('Ad Supported', 'string'),\n",
       " ('In App Purchases', 'string'),\n",
       " ('Editors Choice', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "App Name, App Id, Rating, Rating Count, Released, Last Updated, Content Rating are not interesting here for max installations prediction,\n",
    "so we can remove them safely\n",
    "'''\n",
    "\n",
    "useless_cols= ['App Name','App Id','Rating','Rating Count',\\\n",
    "               'Released','Last Updated','Content Rating','Installs','Minimum Installs']\n",
    "df= remove_useless_col(df,useless_cols)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "happy-aquarium",
   "metadata": {},
   "source": [
    "# Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40e60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_categ_cols= ['Category', 'Ad Supported', 'In App Purchases', 'Editors Choice',\\\n",
    "                  'Free', 'Size', \"Minimum Android\",\"Developer Id\",\"Developer Email\"]\n",
    "\n",
    "df= encode_categ_features(df,interesting_categ_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef5d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange to make label the last column\n",
    "df = df.select('Price','Category', 'Ad Supported', 'In App Purchases', 'Editors Choice',\\\n",
    "            'Free', 'Size', \"Minimum Android\",\"Developer Id\",\"Developer Email\",\\\n",
    "            'Maximum Installs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "critical-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Price', 'Category', 'Ad Supported', 'In App Purchases', 'Editors Choice', 'Free', 'Size', 'Minimum Android', 'Developer Id', 'Developer Email']\n"
     ]
    }
   ],
   "source": [
    "required_features = df.columns[:-1]      #all except the target variable which is at the last index\n",
    "print(required_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outer-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=required_features,outputCol='Features')\n",
    "vec_df = vec_assembler.setHandleInvalid(\"skip\").transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "circular-treaty",
   "metadata": {},
   "source": [
    "# Splitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polished-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = vec_df.select(['Features', 'Maximum Installs']).randomSplit([0.7,0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stupid-cocktail",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "active-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='Features',labelCol='Maximum Installs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atlantic-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5789.517479\n",
      "r2: 0.033304\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "#just some training metrics\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confidential-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------------+\n",
      "|            Features|Maximum Installs|        prediction|\n",
      "+--------------------+----------------+------------------+\n",
      "|(10,[0,1,5,8,9],[...|               8|   68.136875719925|\n",
      "|(10,[0,1,5,8,9],[...|               2|103.00643060375387|\n",
      "|(10,[0,1,5,8,9],[...|             475| 85.52440582609961|\n",
      "|(10,[0,1,5,8,9],[...|             422| 85.47926251734839|\n",
      "|(10,[0,5,6,8,9],[...|              14| 37.53743873067924|\n",
      "+--------------------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_model.transform(test_df)\n",
    "y_pred.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bcc55d9",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continuing-george",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.0319308\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"Maximum Installs\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c4cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5812\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dde13580",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52e76a01",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 9 has 810026 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11848/305052954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'Features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Maximum Installs'\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#setting a larger maxbins(default=32) is an ad-hoc step just to solve an error I dont understand :)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdt_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdt_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m dt_evaluator = RegressionEvaluator(\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 9 has 810026 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(featuresCol ='Features', labelCol = 'Maximum Installs')   #setting a larger maxbins(default=32) is an ad-hoc step just to solve an error I dont understand :)\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Maximum Installs\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "dt_model.featureImportances\n",
    "\n",
    "#notice that Feature at index 0 has higher importance, this feature is the 'Rating Count'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "496bc3dc",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d78d0b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: DecisionTree requires maxBins (= 48) to be at least as large as the number of values in each categorical feature, but categorical feature 9 has 810026 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11848/823251763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgbt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBTRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Maximum Installs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxBins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#same as above, maxbins is set just to avoid an error I dont understand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# maxIter is just a hyperparameter we put by hand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgbt_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mgbt_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbt_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgbt_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Maximum Installs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 48) to be at least as large as the number of values in each categorical feature, but categorical feature 9 has 810026 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'Features', labelCol = 'Maximum Installs', maxIter=10, maxBins=48) #same as above, maxbins is set just to avoid an error I dont understand\n",
    "# maxIter is just a hyperparameter we put by hand\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'Maximum Installs', 'Features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Maximum Installs\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-privacy",
   "metadata": {},
   "source": [
    "### Saving Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this is giving me errors I still cannot resolve\n",
    "\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "# gbt_model.save(\"D:/CLASSSWORKKK/BD/BD_PROJECT/GBT_Model\")\n",
    "# #to overwrite an existing model\n",
    "# gbt_model.write().overwrite().save(\"D:/CLASSSWORKKK/BD/BD_PROJECT/GBT_Model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7399163f",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "# LoadedModel = PipelineModel.load(\"./GBT_Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
